{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in ./.local/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/envs/kernel_personalizado/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in ./.local/lib/python3.10/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/envs/kernel_personalizado/lib/python3.10/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models.keyedvectors import load_word2vec_format\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.jit import RecursiveScriptModule\n",
    "\n",
    "SEED = 2222\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings model to use in the assignment\n",
    "w2v_model = load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> Tuple[List[List[str]], List[int]]:\n",
    "    \"\"\"\n",
    "    Load data from a specified file path, extract texts and targets, and tokenize the texts using the tokenize_tweet function.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], List[int]]: Lists of texts and corresponding targets.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    sentences = []\n",
    "    sentence_labels = []\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    current_sent_idx = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['gold_token']\n",
    "        label = row['gold_label']\n",
    "        sent_idx = row['sent_idx']\n",
    "\n",
    "        if sent_idx != current_sent_idx:\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                sentence_labels.append(current_labels)\n",
    "            current_sentence = []\n",
    "            current_labels = []\n",
    "            current_sent_idx = sent_idx\n",
    "\n",
    "        current_sentence.append(word)\n",
    "        current_labels.append(label)\n",
    "\n",
    "    # Add the last sentence if it is not empty\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        sentence_labels.append(current_labels)\n",
    "\n",
    "    return sentences, sentence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Kenyan, Firms, Eye, Deals, During, Obama, Sum...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[By, Neville, Otuki, Kenya, 's, business, lead...</td>\n",
       "      <td>[0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Industrialists, ,, entrepreneurs, and, banker...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[More, on, This, Kenya, :, Mombasa, Road, ,, U...</td>\n",
       "      <td>[0, 0, 0, 3, 0, 3, 4, 0, 3, 4, 0, 3, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0  [Kenyan, Firms, Eye, Deals, During, Obama, Sum...   \n",
       "1  [By, Neville, Otuki, Kenya, 's, business, lead...   \n",
       "2  [Industrialists, ,, entrepreneurs, and, banker...   \n",
       "3  [More, on, This, Kenya, :, Mombasa, Road, ,, U...   \n",
       "\n",
       "                                              Labels  \n",
       "0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 3, 0, 3, 4, 0, 3, 4, 0, 3, 4, 4, 4, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "tr_texts, tr_targets = load_data('finer_ord_train.csv')\n",
    "vl_texts, vl_targets = load_data('finer_ord_validation.csv')\n",
    "ts_texts, ts_targets = load_data('finer_ord_test.csv')\n",
    "\n",
    "training_data = pd.DataFrame({'Tweets': tr_texts, 'Labels': tr_targets})\n",
    "training_data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        texts (List[List[str]]): List of se tokens.\n",
    "        targets (List[List[int]]): List of target labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 texts: List[List[str]],\n",
    "                 targets: List[List[int]]\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initializes the NERDataset with the given file path.\n",
    "\n",
    "        Args:\n",
    "            texts (List[List[str]]): List of tweets tokens.\n",
    "            targets (List[List[int]]): List of target labels.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[List[str]], List[int]]:\n",
    "        \"\"\"\n",
    "        Returns the embedded tensor and target for the text at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], List[int]]: A tuple containing the texts and the target label for idx.\n",
    "        \"\"\"\n",
    "        return self.texts[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for models\n",
    "tr_dataset = NERDataset(tr_texts, tr_targets)\n",
    "vl_dataset = NERDataset(vl_texts, vl_targets)\n",
    "ts_dataset = NERDataset(ts_texts, ts_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(embedding_model: Any, sentence: List[str], labels:List[int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a sentence to a list of word indices based on an embedding model.\n",
    "\n",
    "    This function iterates through each word in the sentence and retrieves its corresponding index\n",
    "    from the embedding model's vocabulary. If a word is not present in the model's vocabulary,\n",
    "    it is skipped.\n",
    "\n",
    "    Args:\n",
    "        embedding_model (Any): The embedding model with a 'key_to_index' attribute, which maps words to their indices.\n",
    "        sentence (List[str]): A list of words representing the sentence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of word indices corresponding to the words in the tweet.\n",
    "    \"\"\"\n",
    "    index = []\n",
    "    labels_out = [] # We should eliminate labels from the words that do not exist in w2v\n",
    "    for word, label in zip(sentence, labels):\n",
    "        if word in embedding_model.key_to_index:\n",
    "            index.append(embedding_model.key_to_index.get(word))\n",
    "            labels_out.append(label)\n",
    "    \n",
    "    return torch.tensor(index, dtype=torch.long), torch.tensor(labels_out, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model: torch.nn.Module, dataloader: DataLoader, threshold: float = 0.5, device: str = 'cpu') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of a PyTorch model given a DataLoader.\n",
    "\n",
    "    The function moves the model to the specified device, sets it to evaluation mode, and computes\n",
    "    the accuracy by comparing the model's predictions against the true labels. The predictions are\n",
    "    determined based on a specified threshold.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to evaluate.\n",
    "        dataloader (DataLoader): The DataLoader containing the dataset to evaluate against.\n",
    "        threshold (float, optional): Probability threshold to predict a sample as positive. Defaults to 0.5.\n",
    "        device (str, optional): Device to which the model and data are moved ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the given dataset.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels, lenghts in dataloader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(features, torch.Tensor(lenghts))\n",
    "\n",
    "            # output shape: (batch_size, seq_len, num_classes)\n",
    "            # predictions shape: (batch_size, seq_len)\n",
    "            predictions = torch.argmax(torch.softmax(output, dim=2), dim=2)\n",
    "\n",
    "            # mask for valid tokens (label != -1)\n",
    "            mask = labels != -1\n",
    "\n",
    "            correct_predictions += (predictions[mask] == labels[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Tuple[List[str], int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Prepares and returns a batch for training/testing in a torch model.\n",
    "\n",
    "    This function sorts the batch by the length of the text sequences in descending order,\n",
    "    tokenizes the text using a pre-defined word-to-index mapping, pads the sequences to have\n",
    "    uniform length, and converts labels to tensor.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Tuple[List[str], int]]): A list of tuples, where each tuple contains a\n",
    "                                             list of words (representing a text) and an integer label.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three elements:\n",
    "            - texts_padded (torch.Tensor): A tensor of padded word indices of the text.\n",
    "            - labels (torch.Tensor): A tensor of labels.\n",
    "            - lengths (torch.Tensor): A tensor representing the lengths of each text sequence.\n",
    "    \"\"\"\n",
    "    # Sort the batch by the length of text sequences in descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    # Unzip texts and labels from the sorted batch\n",
    "    texts, labels = [text for text, label in sorted_batch], [label for text, label in sorted_batch]\n",
    "\n",
    "    # Convert texts to indices using the word2idx function and w2v_model\n",
    "    texts_indx: List[torch.Tensor] = []\n",
    "    labels_indx: List[torch.Tensor] = []\n",
    "    for sentence, label in zip(texts, labels):\n",
    "        text_indx, label_indx = word2idx(w2v_model, sentence, label)\n",
    "        texts_indx.append(text_indx)\n",
    "        labels_indx.append(label_indx)\n",
    "\n",
    "    # Calculate the lengths of each element of texts_indx.\n",
    "    # The minimum length shall be 1, in order to avoid later problems when training the RNN\n",
    "    lengths: List[torch.Tensor] = [torch.tensor(len(text_indx)) for text_indx in texts_indx]\n",
    "    \n",
    "    # Pad the text sequences to have uniform length\n",
    "    texts_padded: torch.Tensor = torch.nn.utils.rnn.pad_sequence(texts_indx, batch_first=True)\n",
    "    labels_padded: torch.Tensor = torch.nn.utils.rnn.pad_sequence(labels_indx, batch_first=True, padding_value=-1)\n",
    "\n",
    "    return texts_padded, labels_padded, lengths\n",
    "\n",
    "def collate_fn_pre_padding(batch: List[Tuple[List[str], int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    texts, labels = [text for text, label in sorted_batch], [label for text, label in sorted_batch]\n",
    "\n",
    "    texts_indx: List[torch.Tensor] = []\n",
    "    labels_indx: List[torch.Tensor] = []\n",
    "    for sentence, label in zip(texts, labels):\n",
    "        text_indx, label_indx = word2idx(w2v_model, sentence, label)\n",
    "        texts_indx.append(text_indx)\n",
    "        labels_indx.append(label_indx)\n",
    "\n",
    "    max_len = max(len(t) for t in texts_indx)\n",
    "\n",
    "    texts_padded = []\n",
    "    labels_padded = []\n",
    "    lengths = []\n",
    "\n",
    "    for text, label in zip(texts_indx, labels_indx):\n",
    "        pad_len = max_len - len(text)\n",
    "        texts_padded.append(F.pad(text, (pad_len, 0), value=0))        \n",
    "        labels_padded.append(F.pad(label, (pad_len, 0), value=-1))     \n",
    "        lengths.append(torch.tensor(len(text)))\n",
    "\n",
    "    return torch.stack(texts_padded), torch.stack(labels_padded), torch.stack(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "embedding_weights = torch.Tensor(w2v_model.vectors).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration for MLP Classifier training\n",
    "batch_size: int = 64\n",
    "epochs: int = 100\n",
    "print_every: int = 5\n",
    "patience: int = 5\n",
    "learning_rate: float = 0.001\n",
    "hidden_dims: List[int] = 64\n",
    "num_layers: int = 3\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_model(model, train_dataloader, val_dataloader, criterion, optimizer, epochs, print_every, patience, device):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    train_accuracies: Dict[int, float] = {}\n",
    "    val_accuracies: Dict[int, float] = {}\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        torch.cuda.empty_cache()\n",
    "        for i, (x_batch, y_batch, lengths) in enumerate(train_dataloader):\n",
    "            \n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            lengths = torch.Tensor(lengths).to(device)  \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x_batch, lengths)\n",
    "\n",
    "            logits = logits.view(-1, logits.shape[-1])\n",
    "            y_batch = y_batch.view(-1).long()\n",
    "\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch, lengths in val_dataloader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                lengths = torch.Tensor(lengths).to(device) \n",
    "\n",
    "                logits = model(x_batch, lengths)\n",
    "\n",
    "                logits = logits.view(-1, logits.shape[-1])\n",
    "                y_batch = y_batch.view(-1).long()\n",
    "\n",
    "                loss = criterion(logits, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == epochs - 1:\n",
    "            acc_t = calculate_accuracy(model, train_dataloader, device=device)\n",
    "            acc_v = calculate_accuracy(model, val_dataloader, device=device)\n",
    "\n",
    "            train_accuracies[epoch] = acc_t\n",
    "            val_accuracies[epoch] = acc_v\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \")\n",
    "            print(f\"Accuracy: train {acc_t} - test {acc_v}\")\n",
    "            print(f\"Loss: train {avg_train_loss} - test: {avg_val_loss}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "    return train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER(nn.Module):\n",
    "    \"\"\"\n",
    "    A BiLSTM-based NER model with dropout, using pre-trained embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Pre-trained embedding layer.\n",
    "        rnn (nn.LSTM): Bidirectional LSTM layer.\n",
    "        dropout (nn.Dropout): Dropout for regularization.\n",
    "        fc (nn.Linear): Final classification layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_weights: torch.Tensor, hidden_dim: int, num_layers: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_weights (torch.Tensor): Pre-trained embeddings.\n",
    "            hidden_dim (int): Hidden size of LSTM.\n",
    "            num_layers (int): Number of LSTM layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_dim = embedding_weights.shape[1]\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=False)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: bidirectional LSTM → hidden_dim * 2\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 7)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, text_lengths: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        embedded: torch.Tensor = self.embedding(x)\n",
    "\n",
    "        text_lengths = text_lengths.clamp(min=1).cpu()\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        packed_output, _ = self.rnn(packed_embedded)\n",
    "\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        logits = self.fc(output)  # [batch_size, seq_len, num_classes]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RNN classifier model\n",
    "ner_model: NER = NER(embedding_weights=embedding_weights, hidden_dim=hidden_dims, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with specified batch_size, shuffle the training dataloader\n",
    "ner_train_dataloader: DataLoader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=False, drop_last=True)\n",
    "ner_val_dataloader: DataLoader = DataLoader(vl_dataset, batch_size=batch_size, collate_fn=collate_fn, pin_memory=False, drop_last=True)\n",
    "ner_test_dataloader: DataLoader = DataLoader(ts_dataset, batch_size=batch_size, collate_fn=collate_fn, pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter()\n",
    "\n",
    "for x_batch, y_batch, lengths in ner_train_dataloader:\n",
    "    y_batch = y_batch.to(device)\n",
    "    \n",
    "    class_counts.update(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = {label: total_samples / (len(class_counts) * count) for label, count in class_counts.items()}\n",
    "\n",
    "weights = torch.tensor([class_weights[label] for label in range(len(class_counts)-1)], dtype=torch.float32, device=device)\n",
    "\n",
    "ner_criterion = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=-1)\n",
    "ner_optimizer = optim.Adam(ner_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: \n",
      "Accuracy: train 0.669922724383635 - test 0.602393068353734\n",
      "Loss: train 1.690691430568695 - test: 1.4431642691294353\n",
      "Epoch 6/100: \n",
      "Accuracy: train 0.9769061583577713 - test 0.9442992710768807\n",
      "Loss: train 0.07750662580132485 - test: 0.6578282341361046\n",
      "Early stopping triggered at epoch 10\n"
     ]
    }
   ],
   "source": [
    "# Train RNN classifier model\n",
    "name: str = f\"rnn_model_hd{hidden_dims}_nl{num_layers}_batch{batch_size}_epochs{epochs}_dropout2_bidirectional\"\n",
    "writer: SummaryWriter = SummaryWriter(f\"runs/{name}\")\n",
    "ner_train_accuracies, ner_val_accuracies = train_torch_model(ner_model, ner_train_dataloader,\n",
    "                                                             ner_val_dataloader, ner_criterion,\n",
    "                                                             ner_optimizer, epochs, print_every,\n",
    "                                                             patience=patience, device=device)\n",
    " # create folder if it does not exist\n",
    "if not os.path.isdir(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# save scripted model\n",
    "model_scripted: RecursiveScriptModule = torch.jit.script(ner_model.cpu())\n",
    "model_scripted.save(f\"models/{name}.pt\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Model - Training Accuracy: 0.9923367817815316\n",
      "NER Model - Validation Accuracy: 0.9579150048136432\n",
      "NER Model - Test Accuracy: 0.9560214375788146\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy for training, validation and test datasets\n",
    "ner_accuracy_train = calculate_accuracy(ner_model, ner_train_dataloader, device=device)\n",
    "ner_accuracy_val = calculate_accuracy(ner_model, ner_val_dataloader, device=device)\n",
    "ner_accuracy_test =calculate_accuracy(ner_model, ner_test_dataloader, device=device)\n",
    "\n",
    "print(f\"NER Model - Training Accuracy: {ner_accuracy_train}\")\n",
    "print(f\"NER Model - Validation Accuracy: {ner_accuracy_val}\")\n",
    "print(f\"NER Model - Test Accuracy: {ner_accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, labels):\n",
    "    \"\"\"\n",
    "    Calcula las métricas de error para cada clase.\n",
    "    \n",
    "    Args:\n",
    "        y_true (Tensor): Las etiquetas verdaderas (ground truth).\n",
    "        y_pred (Tensor): Las etiquetas predichas.\n",
    "        labels (list): Lista de clases (etiquetas).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame con las métricas de precisión, recall, f1-score por clase.\n",
    "    \"\"\"\n",
    "    y_true = y_true.cpu().numpy() if torch.is_tensor(y_true) else y_true\n",
    "    y_pred = y_pred.cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
    "\n",
    "    report = classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "    \n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    return report_df\n",
    "\n",
    "def evaluate_model(model, dataloader, device, labels):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en un conjunto de datos y calcula las métricas.\n",
    "    \n",
    "    Args:\n",
    "        model: El modelo de PyTorch.\n",
    "        dataloader: El dataloader que proporciona los datos.\n",
    "        device: El dispositivo (CPU o GPU).\n",
    "        labels: Las clases posibles.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Las métricas de precisión, recall, F1, etc. en formato de tabla.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, lengths in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x_batch, torch.Tensor(lengths).to(device))\n",
    "\n",
    "            logits = logits.view(-1, logits.shape[-1])  # [batch_size * seq_len, num_classes]\n",
    "            y_batch = y_batch.view(-1).long()           # [batch_size * seq_len]\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            \n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    metrics_df = compute_metrics(y_true, y_pred, labels)\n",
    "    return metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score  support\n",
      "0              0.999797  0.991787  0.995776  54547.0\n",
      "1              0.974779  0.998708  0.986599    774.0\n",
      "2              0.993802  1.000000  0.996891    481.0\n",
      "3              0.930140  0.998928  0.963307    933.0\n",
      "4              0.001922  1.000000  0.003837    199.0\n",
      "5              0.906853  0.995760  0.949230   1887.0\n",
      "6              0.906994  0.999180  0.950858   1220.0\n",
      "micro avg      0.364800  0.992355  0.533485  60041.0\n",
      "macro avg      0.816327  0.997766  0.835214  60041.0\n",
      "weighted avg   0.990230  0.992355  0.989499  60041.0\n",
      "              precision    recall  f1-score  support\n",
      "0              0.990174  0.977166  0.983627   6394.0\n",
      "1              0.845588  0.851852  0.848708    135.0\n",
      "2              0.887500  0.959459  0.922078     74.0\n",
      "3              0.800000  0.850575  0.824513    174.0\n",
      "4              0.003617  0.844828  0.007202     58.0\n",
      "5              0.693182  0.756198  0.723320    242.0\n",
      "6              0.712264  0.778351  0.743842    194.0\n",
      "micro avg      0.335889  0.957915  0.497376   7271.0\n",
      "macro avg      0.704618  0.859776  0.721899   7271.0\n",
      "weighted avg   0.956724  0.957915  0.953838   7271.0\n",
      "              precision    recall  f1-score  support\n",
      "0              0.992616  0.970048  0.981202  17461.0\n",
      "1              0.822134  0.815686  0.818898    255.0\n",
      "2              0.901961  0.862500  0.881789    160.0\n",
      "3              0.724458  0.847826  0.781302    276.0\n",
      "4              0.001717  0.720000  0.003425     75.0\n",
      "5              0.588410  0.792000  0.675192    500.0\n",
      "6              0.508969  0.744262  0.604527    305.0\n",
      "micro avg      0.361241  0.956021  0.524352  19032.0\n",
      "macro avg      0.648609  0.821760  0.678048  19032.0\n",
      "weighted avg   0.963406  0.956021  0.957364  19032.0\n"
     ]
    }
   ],
   "source": [
    "train_metrics = evaluate_model(ner_model, ner_train_dataloader, device, list(range(7)))\n",
    "val_metrics = evaluate_model(ner_model, ner_val_dataloader, device, list(range(7)))\n",
    "test_metrics = evaluate_model(ner_model, ner_test_dataloader, device, list(range(7)))\n",
    "print(train_metrics)\n",
    "print(val_metrics)\n",
    "print(test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f9e68710aa90e9a31e6326549eb1e2e8f48f8b1eb9c2a824e4113011f4f0058"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
