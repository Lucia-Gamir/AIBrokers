{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALERT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:18:23.389427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-20 16:18:23.399822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745165903.413492    7357 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745165903.417436    7357 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745165903.428244    7357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745165903.428263    7357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745165903.428264    7357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745165903.428265    7357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-20 16:18:23.432608: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers sentencepiece torch\n",
    "# !pip install gensim\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartForConditionalGeneration, GPT2Tokenizer, GPT2LMHeadModel, AutoProcessor, LlavaForConditionalGeneration\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models.keyedvectors import load_word2vec_format\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.jit import RecursiveScriptModule\n",
    "\n",
    "SEED = 2222\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART-LARGE-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen generado:\n",
      " As an experienced investment advisor, provide financial advice based on the following analysis. Advise the investor on how to proceed with their investment in this company, considering the current situation and sentiment.\n"
     ]
    }
   ],
   "source": [
    "# BART\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "input_data = {\n",
    "    \"text\": \"Financial analysts point out that earnings could impact ExxonMobil significantly\",\n",
    "    \"ner\": [('ExxonMobil', 'ORG')],\n",
    "    \"sentiment\": \"positive\"\n",
    "}\n",
    "text = input_data[\"text\"]\n",
    "ner = input_data[\"ner\"]\n",
    "sentiment = input_data[\"sentiment\"]\n",
    "\n",
    "# NER in legible format\n",
    "entity_text = \", \".join([f\"{ent[0]} ({ent[1]})\" for ent in ner])\n",
    "\n",
    "# Input text\n",
    "prompt = (\n",
    "        f\"As an experienced investment advisor, provide financial advice based on the following analysis: \"\n",
    "        f\"'{text}', which has a {sentiment} sentiment towards the company/entity '{entity_text}'. \"\n",
    "        f\"Advise the investor on how to proceed with their investment in this company, considering the current situation and sentiment.\"\n",
    "    )\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nResumen generado:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-SMALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen generado: the investor on how to proceed with their investment in this company, considering the current situation and sentiment.\n"
     ]
    }
   ],
   "source": [
    "# T5-small\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=60,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Resumen generado:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESUMEN: As an experienced investment advisor, provide financial advice based on the following analysis: 'Financial analysts point out that earnings could impact ExxonMobil significantly', which has a positive sentiment towards the company/entity 'ExxonMobil (ORG)'. Advise the investor on how to proceed with their investment in this company, considering the current situation and sentiment.\n",
      "\n",
      "How to Invest in ExxonMobil\n",
      "\n",
      "ExxonMobil is an established company, and has been around for a long time. Its reputation has been\n"
     ]
    }
   ],
   "source": [
    "# GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "input_data = {\n",
    "    \"text\": \"Financial analysts point out that earnings could impact ExxonMobil significantly\",\n",
    "    \"ner\": [('ExxonMobil', 'ORG')],\n",
    "    \"sentiment\": \"positive\"\n",
    "}\n",
    "\n",
    "text = input_data[\"text\"]\n",
    "ner = input_data[\"ner\"]\n",
    "sentiment = input_data[\"sentiment\"]\n",
    "\n",
    "# Legible NER\n",
    "entity_text = \", \".join([f\"{ent[0]} ({ent[1]})\" for ent in ner])\n",
    "\n",
    "prompt = (\n",
    "        f\"As an experienced investment advisor, provide financial advice based on the following analysis: \"\n",
    "        f\"'{text}', which has a {sentiment} sentiment towards the company/entity '{entity_text}'. \"\n",
    "        f\"Advise the investor on how to proceed with their investment in this company, considering the current situation and sentiment.\"\n",
    "    )\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"RESUMEN:\" , generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kernel_personalizado/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Advice:\n",
      " Invest\n"
     ]
    }
   ],
   "source": [
    "# FLAN-T5\n",
    "model_name = \"google/flan-t5-large\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def generate_investment_advice_flan(input_data):\n",
    "    text = input_data[\"text\"]\n",
    "    ner = input_data[\"ner\"]\n",
    "    sentiment = input_data[\"sentiment\"]\n",
    "    caption = input_data[\"caption\"]\n",
    "\n",
    "    entity_text = \", \".join([f\"{ent[0]} ({ent[1]})\" for ent in ner])\n",
    "\n",
    "    prompt = (\n",
    "        f\"Based on the following information, provide investment advice:\\n\\n\"\n",
    "        f\"Text analysis: '{text}'\\n\"\n",
    "        f\"Sentiment: {sentiment}\\n\"\n",
    "        f\"Entity mentioned: '{entity_text}'\\n\"\n",
    "        f\"Image caption: '{caption}'\\n\\n\"\n",
    "        f\"Given this context (take into account positive leads to invest and negative not to invest), what should an investor do regarding this company?\"\n",
    "    )\n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        temperature=0.7,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Input\n",
    "input_data = {\n",
    "    \"text\": \"Financial analysts point out that earnings could impact ExxonMobil significantly\",\n",
    "    \"ner\": [('ExxonMobil', 'ORG')],\n",
    "    \"sentiment\": \"positive\",\n",
    "    \"caption\": None\n",
    "}\n",
    "\n",
    "# Advice\n",
    "advice = generate_investment_advice_flan(input_data)\n",
    "print(\"Generated Advice:\\n\", advice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SA WITH ALERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"GoogleNews-vectors-negative300.bin.gz\", \"rb\") as f_in:\n",
    "    with open(\"GoogleNews-vectors-negative300.bin\", \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model = load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_ner(file_path: str) -> Tuple[List[List[str]], List[int]]:\n",
    "    \"\"\"\n",
    "    Load data from a specified file path, extract texts and targets, and tokenize the texts using the tokenize_tweet function.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], List[int]]: Lists of texts and corresponding targets.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    sentences = []\n",
    "    sentence_labels = []\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    current_sent_idx = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['gold_token']\n",
    "        label = row['gold_label']\n",
    "        sent_idx = row['sent_idx']\n",
    "\n",
    "        if sent_idx != current_sent_idx:\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                sentence_labels.append(current_labels)\n",
    "            current_sentence = []\n",
    "            current_labels = []\n",
    "            current_sent_idx = sent_idx\n",
    "\n",
    "        current_sentence.append(word)\n",
    "        current_labels.append(label)\n",
    "\n",
    "    # The last sentence if it is not empty\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        sentence_labels.append(current_labels)\n",
    "\n",
    "    return sentences, sentence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Kenyan, Firms, Eye, Deals, During, Obama, Sum...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[By, Neville, Otuki, Kenya, 's, business, lead...</td>\n",
       "      <td>[0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Industrialists, ,, entrepreneurs, and, banker...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[More, on, This, Kenya, :, Mombasa, Road, ,, U...</td>\n",
       "      <td>[0, 0, 0, 3, 0, 3, 4, 0, 3, 4, 0, 3, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0  [Kenyan, Firms, Eye, Deals, During, Obama, Sum...   \n",
       "1  [By, Neville, Otuki, Kenya, 's, business, lead...   \n",
       "2  [Industrialists, ,, entrepreneurs, and, banker...   \n",
       "3  [More, on, This, Kenya, :, Mombasa, Road, ,, U...   \n",
       "\n",
       "                                              Labels  \n",
       "0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 3, 0, 3, 4, 0, 3, 4, 0, 3, 4, 4, 4, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_texts, tr_targets = load_data_ner('finer_ord_train.csv')\n",
    "vl_texts, vl_targets = load_data_ner('finer_ord_validation.csv')\n",
    "ts_texts, ts_targets = load_data_ner('finer_ord_test.csv')\n",
    "\n",
    "training_data = pd.DataFrame({'Tweets': tr_texts, 'Labels': tr_targets})\n",
    "training_data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for the dataset.\n",
    "    \n",
    "\n",
    "    Attributes:\n",
    "        texts (List[List[str]]): List of se tokens.\n",
    "        targets (List[List[int]]): List of target labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 texts: List[List[str]],\n",
    "                 targets: List[List[int]]\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initializes the NERDataset with the given file path.\n",
    "\n",
    "        Args:\n",
    "            texts (List[List[str]]): List of tweets tokens.\n",
    "            targets (List[List[int]]): List of target labels.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[List[str]], List[int]]:\n",
    "        \"\"\"\n",
    "        Returns the embedded tensor and target for the text at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], List[int]]: A tuple containing the texts and the target label for idx.\n",
    "        \"\"\"\n",
    "        return self.texts[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = NERDataset(tr_texts, tr_targets)\n",
    "vl_dataset = NERDataset(vl_texts, vl_targets)\n",
    "ts_dataset = NERDataset(ts_texts, ts_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(embedding_model: Any, sentence: List[str], labels:List[int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a sentence to a list of word indices based on an embedding model.\n",
    "\n",
    "    This function iterates through each word in the sentence and retrieves its corresponding index\n",
    "    from the embedding model's vocabulary. If a word is not present in the model's vocabulary,\n",
    "    it is skipped.\n",
    "\n",
    "    Args:\n",
    "        embedding_model (Any): The embedding model with a 'key_to_index' attribute, which maps words to their indices.\n",
    "        sentence (List[str]): A list of words representing the sentence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of word indices corresponding to the words in the tweet.\n",
    "    \"\"\"\n",
    "    index = []\n",
    "    labels_out = [] # We should eliminate labels from the words that do not exist in w2v\n",
    "    for word, label in zip(sentence, labels):\n",
    "        if word in embedding_model.key_to_index:\n",
    "            index.append(embedding_model.key_to_index.get(word))\n",
    "            labels_out.append(label)\n",
    "    \n",
    "    return torch.tensor(index, dtype=torch.long), torch.tensor(labels_out, dtype=torch.long)\n",
    "\n",
    "\n",
    "def calculate_accuracy(model: torch.nn.Module, dataloader: DataLoader, threshold: float = 0.5, device: str = 'cpu') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of a PyTorch model given a DataLoader.\n",
    "\n",
    "    The function moves the model to the specified device, sets it to evaluation mode, and computes\n",
    "    the accuracy by comparing the model's predictions against the true labels. The predictions are\n",
    "    determined based on a specified threshold.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to evaluate.\n",
    "        dataloader (DataLoader): The DataLoader containing the dataset to evaluate against.\n",
    "        threshold (float, optional): Probability threshold to predict a sample as positive. Defaults to 0.5.\n",
    "        device (str, optional): Device to which the model and data are moved ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the given dataset.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels, lenghts in dataloader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(features, torch.Tensor(lenghts))\n",
    "\n",
    "            predictions = torch.argmax(torch.softmax(output, dim=2), dim=2)\n",
    "\n",
    "            # mask for valid tokens (label != -1)\n",
    "            mask = labels != -1\n",
    "\n",
    "            correct_predictions += (predictions[mask] == labels[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Tuple[List[str], int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Prepares and returns a batch for training/testing in a torch model.\n",
    "\n",
    "    This function sorts the batch by the length of the text sequences in descending order,\n",
    "    tokenizes the text using a pre-defined word-to-index mapping, pads the sequences to have\n",
    "    uniform length, and converts labels to tensor.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Tuple[List[str], int]]): A list of tuples, where each tuple contains a\n",
    "                                             list of words (representing a text) and an integer label.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three elements:\n",
    "            - texts_padded (torch.Tensor): A tensor of padded word indices of the text.\n",
    "            - labels (torch.Tensor): A tensor of labels.\n",
    "            - lengths (torch.Tensor): A tensor representing the lengths of each text sequence.\n",
    "    \"\"\"\n",
    "    # Sort the batch by the length of text sequences in descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    # Unzip texts and labels from the sorted batch\n",
    "    texts, labels = [text for text, label in sorted_batch], [label for text, label in sorted_batch]\n",
    "\n",
    "    # Convert texts to indices using the word2idx function and w2v_model\n",
    "    texts_indx: List[torch.Tensor] = []\n",
    "    labels_indx: List[torch.Tensor] = []\n",
    "    for sentence, label in zip(texts, labels):\n",
    "        text_indx, label_indx = word2idx(w2v_model, sentence, label)\n",
    "        texts_indx.append(text_indx)\n",
    "        labels_indx.append(label_indx)\n",
    "\n",
    "    # Calculate the lengths of each element of texts_indx.\n",
    "    # The minimum length shall be 1, in order to avoid later problems when training the RNN\n",
    "    lengths: List[torch.Tensor] = [torch.tensor(len(text_indx)) for text_indx in texts_indx]\n",
    "    \n",
    "    # Pad the text sequences to have uniform length\n",
    "    texts_padded: torch.Tensor = torch.nn.utils.rnn.pad_sequence(texts_indx, batch_first=True)\n",
    "    labels_padded: torch.Tensor = torch.nn.utils.rnn.pad_sequence(labels_indx, batch_first=True, padding_value=-1)\n",
    "\n",
    "    return texts_padded, labels_padded, lengths\n",
    "\n",
    "\n",
    "def collate_fn_pre_padding(batch: List[Tuple[List[str], int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    texts, labels = [text for text, label in sorted_batch], [label for text, label in sorted_batch]\n",
    "\n",
    "    texts_indx: List[torch.Tensor] = []\n",
    "    labels_indx: List[torch.Tensor] = []\n",
    "    for sentence, label in zip(texts, labels):\n",
    "        text_indx, label_indx = word2idx(w2v_model, sentence, label)\n",
    "        texts_indx.append(text_indx)\n",
    "        labels_indx.append(label_indx)\n",
    "\n",
    "    max_len = max(len(t) for t in texts_indx)\n",
    "\n",
    "    texts_padded = []\n",
    "    labels_padded = []\n",
    "    lengths = []\n",
    "\n",
    "    for text, label in zip(texts_indx, labels_indx):\n",
    "        pad_len = max_len - len(text)\n",
    "        texts_padded.append(F.pad(text, (pad_len, 0), value=0))        \n",
    "        labels_padded.append(F.pad(label, (pad_len, 0), value=-1))    \n",
    "        lengths.append(torch.tensor(len(text)))\n",
    "\n",
    "    return torch.stack(texts_padded), torch.stack(labels_padded), torch.stack(lengths)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "embedding_weights = torch.Tensor(w2v_model.vectors).to(\"cpu\")\n",
    "\n",
    "# Define configuration for MLP Classifier training\n",
    "batch_size: int = 64\n",
    "epochs: int = 100\n",
    "print_every: int = 5\n",
    "patience: int = 5\n",
    "learning_rate: float = 0.001\n",
    "hidden_dims: List[int] = 64\n",
    "num_layers: int = 3\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_torch_model(model, train_dataloader, val_dataloader, criterion, optimizer, epochs, print_every, patience, device):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    train_accuracies: Dict[int, float] = {}\n",
    "    val_accuracies: Dict[int, float] = {}\n",
    "\n",
    "    model.to(device) \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        torch.cuda.empty_cache()\n",
    "        for i, (x_batch, y_batch, lengths) in enumerate(train_dataloader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            lengths = torch.Tensor(lengths).to(device) \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x_batch, lengths)\n",
    "\n",
    "            logits = logits.view(-1, logits.shape[-1])\n",
    "            y_batch = y_batch.view(-1).long()\n",
    "\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch, lengths in val_dataloader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                lengths = torch.Tensor(lengths).to(device)  \n",
    "\n",
    "                logits = model(x_batch, lengths)\n",
    "\n",
    "                logits = logits.view(-1, logits.shape[-1])\n",
    "                y_batch = y_batch.view(-1).long()\n",
    "\n",
    "                loss = criterion(logits, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == epochs - 1:\n",
    "            acc_t = calculate_accuracy(model, train_dataloader, device=device)\n",
    "            acc_v = calculate_accuracy(model, val_dataloader, device=device)\n",
    "\n",
    "            train_accuracies[epoch] = acc_t\n",
    "            val_accuracies[epoch] = acc_v\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \")\n",
    "            print(f\"Accuracy: train {acc_t} - test {acc_v}\")\n",
    "            print(f\"Loss: train {avg_train_loss} - test: {avg_val_loss}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "    return train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path: str) -> List[Tuple[str, List[Tuple[str, str]]]]:\n",
    "    \"\"\"\n",
    "    Carga datos con estructura [texto, {entities: [[start, end, label], ...]}]\n",
    "\n",
    "    Devuelve: lista de tuplas (texto, entidades extraídas como (entidad, tipo))\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for item in raw_data:\n",
    "        if not isinstance(item, list) or len(item) != 2:\n",
    "            continue  # Por si hay errores de formato\n",
    "\n",
    "        text = item[0]\n",
    "        entity_info = item[1]\n",
    "        entities = entity_info.get(\"entities\", [])\n",
    "\n",
    "        extracted_entities = [(text[start:end], label) for start, end, label in entities]\n",
    "\n",
    "        data.append((text, extracted_entities))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data(\"finer_test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(embedding_model: Any, text: List[str]) -> torch.Tensor:\n",
    "    indexes = []\n",
    "\n",
    "    for word in text:\n",
    "      if word in embedding_model.key_to_index:\n",
    "        indexes.append(embedding_model.key_to_index[word])\n",
    "\n",
    "    return indexes\n",
    "\n",
    "def preprocess_single_text(text: str, w2v_model, word2idx_fn):\n",
    "    idxs = word2idx_fn(w2v_model, text)\n",
    "    text_tensor = torch.tensor(idxs, dtype=torch.long).unsqueeze(0)\n",
    "    length = torch.tensor([len(idxs)])\n",
    "    return text_tensor, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=(kernel_size - 1) * dilation, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out[:, :, :-self.conv.padding[0]] \n",
    "        out = self.relu(out)\n",
    "        out = self.bn(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class SentimentTCN(torch.nn.Module):\n",
    "    def __init__(self, embedding_weights: torch.Tensor, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        embedding_dim: int = embedding_weights.shape[1]\n",
    "        self.embedding: torch.nn.Embedding = torch.nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "\n",
    "        self.tcn1 = TCNBlock(embedding_dim, 128, kernel_size=3, dilation=1)\n",
    "        self.tcn2 = TCNBlock(128, 128, kernel_size=3, dilation=2)\n",
    "        self.tcn3 = TCNBlock(128, 128, kernel_size=3, dilation=4)\n",
    "\n",
    "        self.pool = torch.nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, text_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        embedded: torch.Tensor = self.embedding(x)          \n",
    "        embedded = embedded.transpose(1, 2)                 \n",
    "\n",
    "        x = self.tcn1(embedded)\n",
    "        x = self.tcn2(x)\n",
    "        x = self.tcn3(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(2)                       \n",
    "        output = self.fc(x)                               \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentTCN(\n",
       "  (embedding): Embedding(3000000, 300)\n",
       "  (tcn1): TCNBlock(\n",
       "    (conv): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (relu): ReLU()\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (tcn2): TCNBlock(\n",
       "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (relu): ReLU()\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (tcn3): TCNBlock(\n",
       "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "    (relu): ReLU()\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (pool): AdaptiveMaxPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_weights = torch.tensor(w2v_model.vectors)\n",
    "sa_model = SentimentTCN(embedding_weights)\n",
    "\n",
    "sa_model.load_state_dict(torch.load(\"sa_model.pt\"))\n",
    "\n",
    "sa_model.eval()\n",
    "sa_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kernel_personalizado/lib/python3.10/site-packages/torch/serialization.py:1434: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = torch.load(\"rnn_model_hd32_nl5_batch65_epochs100_dropout0_bidirectional.pt\", weights_only=False)\n",
    "ner.to(DEVICE)\n",
    "dicc_ner = { 0: \"O\", 1: \"PER_B\" ,  2: \"PER_I\" , 3: \"LOC_B\", 4: \"LOC_I\", 5: \"ORG_B\", 6: \"ORG_I\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alert_image(image):\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"url\": image},\n",
    "                {\"type\": \"text\", \"text\": \"Make a quick and direct conclusion from this graphic.\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        conversation,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, torch.float16)\n",
    "    \n",
    "    # Generate\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "    output = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract string from the list\n",
    "    text = output[0]\n",
    "    description = text.split(\"ASSISTANT:\")[1].strip()\n",
    "    return description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE ALERT FROM A SINGLE TEXT AND IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kernel_personalizado/lib/python3.10/site-packages/torch/nn/modules/module.py:1750: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste the url of the image https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be4e18-6efd-472e-b6eb-fa27f933015b_3600x3600.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d51c6c50924de68ddd58f050fb3a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/opt/conda/envs/kernel_personalizado/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Advice:\n",
      " Not to invest\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "text = \"Amanda Butler, a well-known Director, mentions that losses might influence Burberry's recover.\"\n",
    "url = \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be4e18-6efd-472e-b6eb-fa27f933015b_3600x3600.png\"\n",
    "# text = \"Bitcoin's potential growth: experts suggest positive outlook despite market fluctuations\"\n",
    "# url = \"https://www.economist.com/sites/default/files/images/2021/01/articles/main/20210109_woc295.png\"\n",
    "\n",
    "y_pred = []\n",
    "text_tensor, length = preprocess_single_text(text, w2v_model, word2idx)\n",
    "\n",
    "text_tensor = text_tensor.to(DEVICE)\n",
    "length = length.to(DEVICE)\n",
    "\n",
    "ner_output = ner(text_tensor, torch.tensor([len(text.split(\" \"))]))\n",
    "\n",
    "predicted_classes = torch.argmax(ner_output.squeeze(0), dim=1)\n",
    "\n",
    "words = text.split(\" \")\n",
    "ner_result = [(word, dicc_ner[label.item()]) for word, label in zip(words, predicted_classes) if dicc_ner[label.item()] != \"O\"]\n",
    "\n",
    "entities = ner_result\n",
    "\n",
    "output = sa_model(text_tensor,length)\n",
    "pred = torch.argmax(output, dim=1).item()\n",
    "sentiment = \"positive\" if pred == 1 else \"negative\"\n",
    "\n",
    "caption = generate_alert_image(url)\n",
    "\n",
    "input_data = {\n",
    "\"text\": text,\n",
    "\"ner\": entities,\n",
    "\"sentiment\": sentiment,\n",
    "\"caption\": caption\n",
    "}\n",
    "advice = generate_investment_advice_flan(input_data)\n",
    "print(\"Generated Advice:\\n\", advice)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
